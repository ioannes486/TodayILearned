### Interface

> 유저와 프로그램사이의 연결을 담당함

실제로는 기계와 기계, 시스템과 시스템 사이에서도 수많은 인터페이스를 통해 정보를 주고받고 있다.

### API

> Application Programming Interface

두 소프트웨어가 서로 통신할 수 있게 하는 매커니즘 약속된 방식의 인터페이스

### API의 예시

소셜로그인

날씨 앱

### API Key

> api에게 요청을 보내는 애플리케이션을 구별하기 위한 고유한 식별 문자열로 통신을 안전하게 만들기 위한 핵심 수단

# API사용하기

### 토큰이란?

> gpt와 같은 인공지능 모델에서 텍스트 데이터를 처리하고 이해하는 기본 단위

- 토큰은 문장에서 단어로 생각할 수 있다
- 각 토큰별로 고유한 숫자가 매겨져있다.
- 영어보다 한글 문장을 표현하는데 더 많은 토큰이 소요된다

각 LLM 모델마다 최대로 입력할 수 있느 토큰 수가 제한되어있다.

[토큰 수를 확인할 수 있는 사이트](https://platform.openai.com/tokenizer)

### OpenAI API의 주요 파라미터

**필수 파라미터**

`model`: GPT모델 이름 , ex) gpt-4o-mini

`messages`: 대화 메세지 기록

**응답 다양성 제어**

`temperature`: 다음 토큰 예측의 다양성을 조정한다, 응답의 창의성과 다양성을 조정한다

`top_p`: 선택한 토큰의 확률 범위를 제한(0~1), 누적확률 기반으로 응답의 범위 제한

### 파라미터의 역할 더 자세하게 들여다보기

**temperature**

- 확률 분포를 날카롭거나 평탄하게 만드는 역할
- 높은 확률은 더 높게, 낮은 확률은 더 낮게 조정
- 모든 후보 단어의 확률을 조정하여 다양한 응답을 가능하게

### 질문할 만한 내용?

- temperature와 top_p의 차이?
  **temperature**와 **top_p**는 둘 다 OpenAI가 답변을 만들 때 결과의 "무작위성(다양성)"을 조절하는 중요한 파라미터입니다. 하지만 동작 방식에 큰 차이가 있습니다.
  ## temperature란?
  - **temperature**는 인공지능이 다음에 나올 단어를 얼마나 *자유롭게* 고를지 결정합니다.
  - 값이 **낮을수록(예: 0.2, 0.3)** 가장 가능성이 높은(예상 가능한) 단어를 주로 선택해서 일관성 있고 점잖은 답변이 나옵니다.
  - 값이 **높을수록(예: 1.0, 1.5)** 다양한(예상 밖의) 단어를 쓸 수 있고, 창의적이거나 때로는 엉뚱한 답변이 더 많이 나올 수 있습니다.
  - 즉, **확률 분포의 '날카로움'** 자체를 조절해서 답변의 스타일을 결정한다고 볼 수 있습니다.
  ## top_p란?
  - **top_p**는 AI가 다음 단어를 고를 때, **확률이 높은 단어들만 집단으로 모아 그 안에서 고르겠다고 제한**하는 방식입니다.
  - 예를 들어 top_p=0.8 설정 시, 다음 단어 후보들을 확률 순으로 나열해서 누적 확률이 0.8(80%)이 될 때까지 모은 후 그 집단 중에서 무작위로 단어를 뽑는 방식입니다.
  - 값을 **낮게** 하면(예: 0.3, 0.5) 후보군이 적어져서 더 뻔하고 일관적인 답이 잘 나오고, **높게** 하면(예: 0.9, 1.0) 후보군이 넓어져서 보다 다양한 답이 나올 수 있습니다.
  ## 한눈에 보는 차이점
  | **파라미터**    | **의미**                            | **작동 방식**                          | **추천 용도**                    |
  | --------------- | ----------------------------------- | -------------------------------------- | -------------------------------- |
  | **temperature** | 다음 단어를 고를 때의 '창의성' 조절 | 확률 분포의 곡선을 날카롭거나 평평하게 | 글의 분위기·톤 조정, 창발성 추가 |
  | **top_p**       | 후보 단어 집합의 다양성 크기 조절   | 누적 확률로 상위 비율만 후보로 제한    | 결과의 일관성·다양성 균형 설정   |
  > 두 값을 동시에 조정하는 건 추천하지 않고, 하나만 바꿔가면서 테스트해보는 것을 권장합니다.
  ## 중학생 수준 요약
  - **temperature** : "AI야, 평소대로 말할래, 아니면 좀 더 엉뚱하게 말해볼래?" 하는 창의성/무작위성 조절용 다이얼
  - **top_p** : "AI야, 고를 수 있는 단어를 확률 높은 것끼리만 모아서, 그 안에서 골라!" 하는 후보군 크기 제한 스위치
  **정리:**
  둘 다 답변의 스타일(일관성/다양성/창의성)에 큰 영향을 주지만, *temperature*는 단어를 고르는 '자유로움' 자체를, *top_p*는 후보군의 '범위'를 각각 조절합니다. 동시에 높이면 지나치게 엉뚱하거나 비논리적인 답변이 나올 수 있으니 한 번에 하나만 조정하는 것이 좋습니다.
- 사용자가 답변 길이를 설정하지 않았을 때 챗봇은 어떻게 임의로 답변길이를 설정할까?
  ## 답변 길이 자동 결정 알고리즘의 원리
  챗봇이나 AI가 사용자가 별도로 요청하지 않았음에도 때에 따라 **긴 답변**과 **짧은 답변**을 구분해 내는 것은 모델의 내부 논리와 학습 방식, 그리고 추가 알고리즘에 기반합니다. 아래에 주요 접근 방식을 소개합니다.
  ## 1. 질의 유형 분류 (Query Classification)
  - **정보성 질문**(예: "피타고라스의 정리를 설명해줘"):
    - 주로 긴 설명이 필요한 내용으로 분류
  - **간단 사실 질문**(예: "파이 값은?"):
    - 짧은 답변이 적합
  - AI는 기계학습을 통해 입력 질문의 **패턴, 길이, 문장 구조, 특정 키워드** 등을 분석하여 질문 유형을 분류하고, 답변의 적절한 길이를 예측합니다.
  ## 2. 컨텍스트 및 명시성 분석
  - **맥락 단서 활용**:
    - 질문에 "간단히", "자세히"와 같은 단어가 포함된 경우 길이 조절
  - **과거 대화 히스토리**:
    - 이전 대화에서 짧은 응답 선호도가 높았던 경우 똑같이 반영
  - **질문 자체의 복잡성**:
    - 여러 세부항목이 있거나, 복합적인 설명이 요구될 경우 길게 답변
  ## 3. 길이 예측과 적응적 보상 모델
  - **모델에서 길이 예측**:
    - 최신 연구는 AI 자신이 응답을 생성하기 전에 “내 답변이 어느 정도 길이일지”를 미리 추론하도록 하거나, 입력 질문을 분석해 적당한 토큰 수를 예측하는 방식을 발전시켰습니다.
  - **적응적 보상(Adaptive Reward) 적용**:
    - AI 학습 과정에서 사람 피드백을 받아 “어떤 유형의 질문에 어느 정도 길이의 답변이 만족스러운가?”를 학습합니다.
    - 최근에는 질문의 성격에 따라 답변 길이의 중요도를 **동적으로 조절(Adaptive Length Bias Mitigation, ALBM)**하는 보상 모델이 연구되고 있습니다. 즉, 질문의 종류별로 적정 답변 길이에 차등을 두어 알고리즘이 결과물의 길이를 조절합니다.
  ## 4. 수학적·통계학적 원리
  - 주로 **분류(Classification)**와 **회귀(Regression)**, 그리고 토큰 길이의 **예측(Prediction)**이 결합됩니다.
    - 분류: 질문 유형을 카테고리로 나눔
    - 회귀: 예상되는 답변 길이(토큰 수)를 수치로 예측
    - 학습 데이터에서 질문-답변 쌍을 통해, 통계적으로 어떤 질문에 얼마나 긴 답변이 적합한지 학습
  - 적용되는 수식 및 원리:
    - 입력 토큰 → 분류기/회귀모델 → 적정 답변 길이 산출
    - (실제 연구 중에는 “Perception in Advance(PiA)”와 같은 방식도 도입)
  ## 요약 정리
  - AI는 질문의 종류, 복잡성, 명확성, 과거 대화 맥락 등 여러 신호를 종합하여 “길게” 또는 “짧게” 답변할지를 내부적으로 자동 결정합니다.
  - 이 과정에는 머신러닝 기반 분류·회귀 모델, 적응적 보상 모델, 길이 예측 등 복합 알고리즘이 함께 작동합니다.
  - 최근에는 응답 생성 전에 예상 길이를 미리 추정하는 연구(PiA), 보상모델에서 길이의 영향력을 질문 특성에 따라 다르게 조정하는 방법(ALBM)도 등장하고 있습니다.
  _즉, 특별히 명명된 단일 공식이 있다기보다, 분류·회귀·적응적 길이 제어가 결합된 최신 자연어처리 알고리즘들의 조합이라 할 수 있습니다._
